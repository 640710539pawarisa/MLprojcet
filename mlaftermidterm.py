# -*- coding: utf-8 -*-
"""MLAfterMidterm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LwyyvKqOVziakvLHrJfd4LM50J-Jrr1g
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('creditcard.csv')
df

df.describe()

df.isnull().sum()

# Features
X = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',
        'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]
# Target
y = df['Class']
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)



#feature selection
#RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize the model (Logistic Regression or Random Forest)
model = LogisticRegression()
# OR
# model = RandomForestClassifier(random_state=42)

# Apply RFE with the estimator
rfe = RFE(estimator=model, n_features_to_select=10)  # Choose how many features to select
X_rfe = df.drop(columns=['Class'], axis=1)
y = df['Class']

rfe.fit_transform(X_rfe, y)

# Create a list of features with their ranking and selection status
features = [(col, rfe.support_[i], rfe.ranking_[i]) for i, col in zip(range(X_rfe.shape[1]), X_rfe.columns)]

# Sort the features by ranking
features_sorted_by_rank = sorted(features, key=lambda x: x[2])

# Print the features sorted by rank
for feature in features_sorted_by_rank:
    print(f"Feature: {feature[0]} selected: {feature[1]} rank={feature[2]}")

#Anova(filter)

from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X is your feature matrix and y is the target (Class column)
# Here we select the top 10 features based on ANOVA F-score
k_best = SelectKBest(score_func=f_classif, k=10)
X_selected = k_best.fit_transform(X, y)

# Get the scores for each feature
anova_scores = k_best.scores_

# Create a DataFrame to map feature names to their scores
feature_names = X.columns
anova_df = pd.DataFrame({'Feature': feature_names, 'F-Score': anova_scores})

# Sort the features by their ANOVA F-score in descending order
anova_df = anova_df.sort_values(by='F-Score', ascending=False)

# Display the top selected features based on ANOVA F-Score
print(anova_df.head(10))  # Top 10 features
# Plot the ANOVA F-scores for the top features
plt.figure(figsize=(10, 6))
sns.barplot(x='F-Score', y='Feature', data=anova_df.head(100), palette='viridis')
plt.title('Top 10 Features by ANOVA F-Score')
plt.xlabel('ANOVA F-Score')
plt.ylabel('Feature')
plt.show()

#Lasso regression(embedded)
from sklearn.linear_model import Lasso
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Standardize your features if necessary
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit Lasso model (alpha is the regularization strength)
lasso = Lasso(alpha=0.00001)  # Adjust alpha as needed
lasso.fit(X_scaled, y)

# Get the coefficients
lasso_coef = lasso.coef_

# Create a DataFrame to show feature names and their coefficients
lasso_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso_coef})

# Display the features with non-zero coefficients (important features)
selected_features = lasso_df[lasso_df['Coefficient'] != 0]
print(selected_features)
# Sort the features by their absolute coefficient values
lasso_df['Absolute Coefficient'] = np.abs(lasso_df['Coefficient'])
lasso_df = lasso_df.sort_values(by='Absolute Coefficient', ascending=False)

# Plot the coefficients
plt.figure(figsize=(10, 6))
sns.barplot(x='Absolute Coefficient', y='Feature', data=lasso_df)
plt.title('Feature Importance Based on Lasso Coefficients')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

# Check for imbalance
print(df['Class'].value_counts())

#smote
# Handle data imbalance using SMOTE

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)
sm
# Confirming the new class distribution after SMOTE

print(pd.Series(y_res).value_counts())

#Split data

# Features
X = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',
        'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]
# Target
y = df['Class']

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2)

# Check the size of the split

print("X_res shape:", X_res.shape)

print("X_train size:", X_train.shape) #80%×568,630 (Training samples)
print("X_test size:", X_test.shape)   #20%×568,630 (Testing samples)

import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import make_scorer, f1_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

# Load dataset
data = pd.read_csv('creditcard.csv')
X = data.drop('Class', axis=1)
y = data['Class']

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

# Define SMOTE and scaling
smote = SMOTE()

# Cross-validate each model with SMOTE and scaling using 5-fold cross-validation
for name, model in models.items():
    pipeline = Pipeline([('smote', smote), ('scaler', StandardScaler()), ('model', model)])
    f1 = cross_val_score(pipeline, X, y, cv=StratifiedKFold(5), scoring=make_scorer(f1_score, average='macro'))
    print(f"{name} F1-score: {f1.mean():.4f} (+/- {f1.std():.4f})")

#model

#1.svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#: Initialize the SVM model
svm_model = SVC(kernel='rbf', random_state=42)  # other kernels like 'linear', 'poly', etc.

#: Train the model
svm_model.fit(X_train_scaled, y_train)

#: Make predictions
y_pred = svm_model.predict(X_test_scaled)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#2.KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#: Initialize the KNN model
knn_model = KNeighborsClassifier(n_neighbors=3)

#: Train the model
knn_model.fit(X_train, y_train)

#: Make predictions
y_pred = knn_model.predict(X_test)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#3.logistic
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Create an instance of the Logistic Regression model
logistic_model = LogisticRegression()

#: Train the model
logistic_model.fit(X_train, y_train)

#: Make predictions
y_pred = logistic_model.predict(X_test)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#4.random forest
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

# Initialize the Random Forest Classifier
rf_model = RandomForestClassifier()

#: Train the model
rf_model.fit(X_train, y_train)

#: Make predictions
y_pred = rf_model.predict(X_test)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#model
from sklearn.neural_network import MLPClassifier

#5.ANN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

# Initialize
#X,y=make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)
clf=MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)

#: Train the model
clf.fit(X_train, y_train)

#: Make predictions
y_pred = clf.predict(X_test)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#6 Decision tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
# Initialize
model=DecisionTreeClassifier()
#: Train the model
model.fit(X_train,y_train)

#: Make predictions
y_pred=model.predict(X_test)

#: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix=confusion_matrix(y_test,y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print('Confusion Matrix: ')
print(conf_matrix)
print("Classification Report:\n", report)